{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0d36c0",
   "metadata": {},
   "source": [
    "# Model_X (version 3)\n",
    "\n",
    "after the previous version, we noticed that this data set consists more than single row per the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00875a8",
   "metadata": {},
   "source": [
    "# 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4beba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data. Shape: (195196, 1024)\n",
      "Target column 'DEMENTED' found.\n",
      "Group column 'NACCID' found.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Install Necessary Libraries ---\n",
    "# (Run these lines once if you don't have them in your new environment)\n",
    "# !pip install xgboost catboost shap\n",
    "\n",
    "# --- 2. Import All Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Model building and evaluation\n",
    "# --- We now import GroupShuffleSplit for the patient-aware split ---\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# --- 3. Load Data ---\n",
    "# IMPORTANT: Update this path to where your CSV file is located\n",
    "DATA_FILE_PATH = './Dataset/Dementia Prediction Dataset.csv' \n",
    "TARGET_COLUMN = 'DEMENTED'\n",
    "GROUP_COLUMN = 'NACCID'  # This is the Patient ID\n",
    "\n",
    "try:\n",
    "    # Use low_memory=False to avoid DtypeWarning during dtype inference.\n",
    "    data = pd.read_csv(DATA_FILE_PATH, low_memory=False)\n",
    "    print(f\"Successfully loaded data. Shape: {data.shape}\")\n",
    "    print(f\"Target column '{TARGET_COLUMN}' found.\")\n",
    "    print(f\"Group column '{GROUP_COLUMN}' found.\")\n",
    "except (FileNotFoundError, KeyError):\n",
    "    print(f\"Error: Could not load data or find all necessary columns.\")\n",
    "    print(\"Please check DATA_FILE_PATH, TARGET_COLUMN, and GROUP_COLUMN variables.\")\n",
    "    # Create a small dummy dataset for demonstration\n",
    "    data = pd.DataFrame({\n",
    "        'NACCID': ['A', 'A', 'B', 'C', 'C', 'C', 'D', 'E', 'E', 'F'],\n",
    "        'NACCAGE': [65, 66, 70, 80, 81, 82, 75, 999, 68, 69], \n",
    "        'SEX': [1, 1, 2, 1, 1, 1, 2, 1, 2, 1], \n",
    "        'EDUC': [12, 12, 16, 8, 8, 8, 20, 99, 14, 16],\n",
    "        'MARISTAT': [1, 1, 2, 1, 1, 5, 5, 9, 2, 1], \n",
    "        'RACE': [1, 1, 1, 2, 2, 2, 5, 99, 1, 1], \n",
    "        'INRELTO': [1, 1, 2, 9, 3, 3, 3, 1, 1, 2],\n",
    "        'CVHATT': [0, 0, 1, 2, 2, 2, 0, 9, 0, 1], \n",
    "        'CBSTROKE': [0, 0, 0, 1, 1, 1, 0, 9, 0, 0], \n",
    "        'DIABETES': [0, 1, 2, 1, 1, 1, 0, 9, 1, 0],\n",
    "        'HYPERTEN': [1, 1, 1, 0, 0, 0, 0, 9, 1, 1], \n",
    "        'HYPERCHO': [1, 1, 0, 2, 2, 9, 0, 0, 1, 0], \n",
    "        'TBI': [0, 0, 0, 0, 0, 0, 0, 9, 1, 0], \n",
    "        'DEP2YRS': [0, 0, 1, 0, 0, 0, 0, 9, 1, 0], \n",
    "        'NACCBMI': [25.1, 25.5, 28.9, 32.0, 32.1, 32.2, 22.4, 888.8, 26.0, 27.0],\n",
    "        'DEMENTED': [0, 0, 1, 1, 1, 1, 0, 1, 0, 1]\n",
    "    })\n",
    "    print(f\"Loaded dummy data for demonstration. Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce266c4",
   "metadata": {},
   "source": [
    "# 2. Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa70b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced special 'missing' codes with NaN.\n",
      "Engineered 'ComorbidityCount' feature.\n",
      "\n",
      "--- Preprocessing Complete ---\n",
      "Shape of processed features (X): (195196, 26)\n",
      "Shape of target (y): (195196,)\n",
      "Shape of groups (groups): (195196,)\n",
      "\n",
      "Processed features (first 5 rows):\n",
      "   num__NACCAGE  num__EDUC  num__NACCBMI  num__ComorbidityCount  cat__SEX_1.0  \\\n",
      "0     -0.424061   0.136566      0.831051               1.549393           1.0   \n",
      "1     -0.327002   0.136566      0.683279              -0.858180           1.0   \n",
      "2     -0.812297   0.136566      0.074805              -0.858180           1.0   \n",
      "3     -1.103474   0.136566      0.222577              -0.858180           0.0   \n",
      "4      0.255351  -1.107783     -0.333743              -0.858180           1.0   \n",
      "\n",
      "   cat__SEX_2.0  cat__MARISTAT_1.0  cat__MARISTAT_2.0  cat__MARISTAT_3.0  \\\n",
      "0           0.0                1.0                0.0                0.0   \n",
      "1           0.0                1.0                0.0                0.0   \n",
      "2           0.0                1.0                0.0                0.0   \n",
      "3           1.0                1.0                0.0                0.0   \n",
      "4           0.0                0.0                0.0                1.0   \n",
      "\n",
      "   cat__MARISTAT_4.0  ...  cat__RACE_5.0  cat__RACE_50.0  cat__INRELTO_-4.0  \\\n",
      "0                0.0  ...            0.0             0.0                0.0   \n",
      "1                0.0  ...            0.0             0.0                0.0   \n",
      "2                0.0  ...            0.0             0.0                0.0   \n",
      "3                0.0  ...            0.0             0.0                1.0   \n",
      "4                0.0  ...            0.0             0.0                0.0   \n",
      "\n",
      "   cat__INRELTO_1.0  cat__INRELTO_2.0  cat__INRELTO_3.0  cat__INRELTO_4.0  \\\n",
      "0               1.0               0.0               0.0               0.0   \n",
      "1               1.0               0.0               0.0               0.0   \n",
      "2               1.0               0.0               0.0               0.0   \n",
      "3               0.0               0.0               0.0               0.0   \n",
      "4               0.0               0.0               1.0               0.0   \n",
      "\n",
      "   cat__INRELTO_5.0  cat__INRELTO_6.0  cat__INRELTO_7.0  \n",
      "0               0.0               0.0               0.0  \n",
      "1               0.0               0.0               0.0  \n",
      "2               0.0               0.0               0.0  \n",
      "3               0.0               0.0               0.0  \n",
      "4               0.0               0.0               0.0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Feature Sets ---\n",
    "# Based on hackathon rules: A1, A2, A5, B1 are allowed.\n",
    "\n",
    "# Numerical features that will be imputed (median) and scaled\n",
    "numeric_features = [\n",
    "    'NACCAGE',  # Subject's age\n",
    "    'EDUC',     # Subject's years of education\n",
    "    'NACCBMI'   # Subject's Body Mass Index\n",
    "]\n",
    "\n",
    "# Categorical features that will be imputed (most frequent) and one-hot encoded\n",
    "categorical_features = [\n",
    "    'SEX',      # Subject's sex\n",
    "    'MARISTAT', # Marital status\n",
    "    'RACE',     # Subject's race\n",
    "    'INRELTO'   # Co-participant's relationship to subject\n",
    "]\n",
    "\n",
    "# Self-reported health history (Form A5)\n",
    "# We will binarize these and engineer a new feature\n",
    "health_history_features = [\n",
    "    'CVHATT',   # Heart attack [cite: 32]\n",
    "    'CBSTROKE', # Stroke [cite: 32]\n",
    "    'DIABETES', # Diabetes [cite: 34]\n",
    "    'HYPERTEN', # Hypertension [cite: 34]\n",
    "    'HYPERCHO', # Hypercholesterolemia [cite: 34]\n",
    "    'TBI',      # Traumatic Brain Injury [cite: 34]\n",
    "    'DEP2YRS'   # Depression in last 2 years [cite: 36]\n",
    "]\n",
    "\n",
    "# --- 2. Clean Missing/Unknown Values ---\n",
    "# We replace all non-standard \"missing\" or \"unknown\" codes with np.nan\n",
    "# This is critical for scikit-learn's imputers to work.\n",
    "missing_values_map = {\n",
    "    'NACCAGE': [999],           # [cite: 420]\n",
    "    'EDUC': [99],               # [cite: 367]\n",
    "    'NACCBMI': [888.8, 888],    # [cite: 1118]\n",
    "    'MARISTAT': [9],            # [cite: 377]\n",
    "    'RACE': [99],               # [cite: 295]\n",
    "    'INRELTO': [9],             # [cite: 588]\n",
    "    # For Form A5, 9 means \"Unknown\"\n",
    "    'CVHATT': [9], 'CBSTROKE': [9], 'DIABETES': [9], 'HYPERTEN': [9], # [cite: 1094, 1098, 1105, 1106]\n",
    "    'HYPERCHO': [9], 'TBI': [9], 'DEP2YRS': [9] # [cite: 1107, 1102, 1115]\n",
    "}\n",
    "\n",
    "for col, missing_vals in missing_values_map.items():\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].replace(missing_vals, np.nan)\n",
    "\n",
    "print(\"Replaced special 'missing' codes with NaN.\")\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "# 3a. Binarize Health History\n",
    "# We map 0=Absent to 0, and 1=Recent/Active or 2=Remote/Inactive to 1 (Present).\n",
    "for col in health_history_features:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].map({0: 0, 1: 1, 2: 1})\n",
    "\n",
    "# 3b. Create 'ComorbidityCount'\n",
    "# This counts how many conditions are present.\n",
    "# We fill NaNs with 0 (assuming 'unknown' means 'absent') before summing.\n",
    "data['ComorbidityCount'] = data[health_history_features].fillna(0).sum(axis=1)\n",
    "\n",
    "# Add our new engineered feature to the numeric list\n",
    "numeric_features.append('ComorbidityCount')\n",
    "print(\"Engineered 'ComorbidityCount' feature.\")\n",
    "\n",
    "# --- 4. Separate Features (X) and Target (y) ---\n",
    "if TARGET_COLUMN not in data.columns:\n",
    "    print(f\"FATAL ERROR: Target column '{TARGET_COLUMN}' not found!\")\n",
    "else:\n",
    "    # X contains the raw features to be processed\n",
    "    X = data[numeric_features + categorical_features]\n",
    "    # y contains the final target\n",
    "    y = data[TARGET_COLUMN]\n",
    "    # groups contains the Patient IDs for the split\n",
    "    groups = data[GROUP_COLUMN]\n",
    "\n",
    "    # --- 5. Define Preprocessing Pipelines ---\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # --- 6. Create the ColumnTransformer ---\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_pipeline, numeric_features),\n",
    "            ('cat', categorical_pipeline, categorical_features)\n",
    "        ],\n",
    "        remainder='drop' # Drop any columns we didn't explicitly select\n",
    "    )\n",
    "\n",
    "    # --- 7. Apply Preprocessing ---\n",
    "    # Fit and transform the entire dataset to prepare it\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Get the new feature names (e.g., after one-hot encoding)\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    \n",
    "    # Convert the processed data back to a DataFrame (useful for SHAP later)\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "    print(\"\\n--- Preprocessing Complete ---\")\n",
    "    print(f\"Shape of processed features (X): {X_processed.shape}\")\n",
    "    print(f\"Shape of target (y): {y.shape}\")\n",
    "    print(f\"Shape of groups (groups): {groups.shape}\")\n",
    "    print(\"\\nProcessed features (first 5 rows):\")\n",
    "    print(X_processed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca8968",
   "metadata": {},
   "source": [
    "# 3. Patient-Aware data Splitting\n",
    "\n",
    "We are now using GroupShuffleSplit to ensure that all records (visits) for any given patient (NACCID) are kept together in either the training set or the testing set, but never split between both.\n",
    "\n",
    "This prevents data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acdfd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Patient-Aware (Grouped) Split ---\n",
      "--- Data Splitting Complete ---\n",
      "Total rows:      195196\n",
      "Training rows:   155758\n",
      "Testing rows:    39438\n",
      "\n",
      "Checking for patient overlap (data leakage)...\n",
      "SUCCESS: No patients are shared between train and test sets.\n",
      "Total patients:    52537\n",
      "Training patients: 42029\n",
      "Testing patients:  10508\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Set up the GroupShuffleSplit ---\n",
    "# This splitter ensures that all rows for a single 'group' (NACCID)\n",
    "# stay in the same set (either train or test).\n",
    "\n",
    "print(\"--- Preparing Patient-Aware (Grouped) Split ---\")\n",
    "\n",
    "# We create one split (n_splits=1) with 20% of the *groups* (patients) \n",
    "# held out for the test set.\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# gss.split() returns the *indices* of the rows for train/test\n",
    "# We need to get the first (and only) split from the generator\n",
    "train_idx, test_idx = next(gss.split(X_processed, y, groups=groups))\n",
    "\n",
    "# --- 2. Create the Training and Testing Sets ---\n",
    "# We use the indices to select the correct rows from our processed data\n",
    "\n",
    "X_train = X_processed[train_idx]\n",
    "y_train = y.iloc[train_idx]  # Use .iloc for pandas Series\n",
    "\n",
    "X_test = X_processed[test_idx]\n",
    "y_test = y.iloc[test_idx]    # Use .iloc for pandas Series\n",
    "\n",
    "# --- 3. Report on the Split ---\n",
    "print(\"--- Data Splitting Complete ---\")\n",
    "print(f\"Total rows:      {len(X_processed)}\")\n",
    "print(f\"Training rows:   {len(X_train)}\")\n",
    "print(f\"Testing rows:    {len(X_test)}\")\n",
    "print(\"\\nChecking for patient overlap (data leakage)...\")\n",
    "\n",
    "train_patients = set(groups.iloc[train_idx])\n",
    "test_patients = set(groups.iloc[test_idx])\n",
    "overlap = train_patients.intersection(test_patients)\n",
    "\n",
    "if len(overlap) == 0:\n",
    "    print(\"SUCCESS: No patients are shared between train and test sets.\")\n",
    "    print(f\"Total patients:    {len(train_patients) + len(test_patients)}\")\n",
    "    print(f\"Training patients: {len(train_patients)}\")\n",
    "    print(f\"Testing patients:  {len(test_patients)}\")\n",
    "else:\n",
    "    print(f\"ERROR: {len(overlap)} patients are in BOTH train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2b719",
   "metadata": {},
   "source": [
    "# 4. Model Development & Comparison\n",
    "\n",
    "re-run the all four models we have used in previous versions (in 1 & 2 notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc44fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression (Patient-Aware Split) ---\n",
      "Logistic Regression Accuracy: 0.7287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.83     28023\n",
      "           1       0.59      0.21      0.31     11415\n",
      "\n",
      "    accuracy                           0.73     39438\n",
      "   macro avg       0.67      0.58      0.57     39438\n",
      "weighted avg       0.70      0.73      0.68     39438\n",
      "\n",
      "----------------------------------------\n",
      "--- Training Random Forest (Patient-Aware Split) ---\n",
      "Random Forest Accuracy: 0.6932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79     28023\n",
      "           1       0.46      0.38      0.42     11415\n",
      "\n",
      "    accuracy                           0.69     39438\n",
      "   macro avg       0.61      0.60      0.60     39438\n",
      "weighted avg       0.68      0.69      0.68     39438\n",
      "\n",
      "----------------------------------------\n",
      "--- Training XGBoost Classifier (Patient-Aware Split) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metal/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:45:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83     28023\n",
      "           1       0.59      0.31      0.40     11415\n",
      "\n",
      "    accuracy                           0.74     39438\n",
      "   macro avg       0.68      0.61      0.62     39438\n",
      "weighted avg       0.71      0.74      0.71     39438\n",
      "\n",
      "----------------------------------------\n",
      "--- Training CatBoost Classifier (Patient-Aware Split) ---\n",
      "CatBoost Accuracy: 0.7380\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83     28023\n",
      "           1       0.59      0.31      0.41     11415\n",
      "\n",
      "    accuracy                           0.74     39438\n",
      "   macro avg       0.68      0.61      0.62     39438\n",
      "weighted avg       0.71      0.74      0.71     39438\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- üèÜ Model Performance Summary (Patient-Aware) ---\n",
      "Logistic Regression: 0.7287\n",
      "Random Forest:       0.6932\n",
      "XGBoost:             0.7369\n",
      "CatBoost:            0.7380\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Train Baseline Model: Logistic Regression ---\n",
    "print(\"--- Training Logistic Regression (Patient-Aware Split) ---\")\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_preds = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_preds)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 2. Train Random Forest Classifier ---\n",
    "print(\"--- Training Random Forest (Patient-Aware Split) ---\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(classification_report(y_test, rf_preds))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 3. Train XGBoost Classifier ---\n",
    "print(\"--- Training XGBoost Classifier (Patient-Aware Split) ---\")\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(classification_report(y_test, xgb_preds))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Train CatBoost Classifier ---\n",
    "print(\"--- Training CatBoost Classifier (Patient-Aware Split) ---\")\n",
    "cat_model = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "    verbose=0  # Suppress training output\n",
    ")\n",
    "cat_model.fit(X_train, y_train)\n",
    "cat_preds = cat_model.predict(X_test)\n",
    "cat_accuracy = accuracy_score(y_test, cat_preds)\n",
    "\n",
    "print(f\"CatBoost Accuracy: {cat_accuracy:.4f}\")\n",
    "print(classification_report(y_test, cat_preds))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 5. Final Model Comparison (Patient-Aware) ---\n",
    "print(\"\\n--- üèÜ Model Performance Summary (Patient-Aware) ---\")\n",
    "print(f\"Logistic Regression: {lr_accuracy:.4f}\")\n",
    "print(f\"Random Forest:       {rf_accuracy:.4f}\")\n",
    "print(f\"XGBoost:             {xgb_accuracy:.4f}\")\n",
    "print(f\"CatBoost:            {cat_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
