{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0d36c0",
   "metadata": {},
   "source": [
    "# Model_X (version 3)\n",
    "\n",
    "after the previous version, we noticed that this data set consists more than single row per the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00875a8",
   "metadata": {},
   "source": [
    "# 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4beba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data. Shape: (195196, 1024)\n",
      "Target column 'DEMENTED' found.\n",
      "Group column 'NACCID' found.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Install Necessary Libraries ---\n",
    "# (Run these lines once if you don't have them in your new environment)\n",
    "# !pip install xgboost catboost shap\n",
    "\n",
    "# --- 2. Import All Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Model building and evaluation\n",
    "# --- We now import GroupShuffleSplit for the patient-aware split ---\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# --- 3. Load Data ---\n",
    "# IMPORTANT: Update this path to where your CSV file is located\n",
    "DATA_FILE_PATH = './Dataset/Dementia Prediction Dataset.csv' \n",
    "TARGET_COLUMN = 'DEMENTED'\n",
    "GROUP_COLUMN = 'NACCID'  # This is the Patient ID\n",
    "\n",
    "try:\n",
    "    # Use low_memory=False to avoid DtypeWarning during dtype inference.\n",
    "    data = pd.read_csv(DATA_FILE_PATH, low_memory=False)\n",
    "    print(f\"Successfully loaded data. Shape: {data.shape}\")\n",
    "    print(f\"Target column '{TARGET_COLUMN}' found.\")\n",
    "    print(f\"Group column '{GROUP_COLUMN}' found.\")\n",
    "except (FileNotFoundError, KeyError):\n",
    "    print(f\"Error: Could not load data or find all necessary columns.\")\n",
    "    print(\"Please check DATA_FILE_PATH, TARGET_COLUMN, and GROUP_COLUMN variables.\")\n",
    "    # Create a small dummy dataset for demonstration\n",
    "    data = pd.DataFrame({\n",
    "        'NACCID': ['A', 'A', 'B', 'C', 'C', 'C', 'D', 'E', 'E', 'F'],\n",
    "        'NACCAGE': [65, 66, 70, 80, 81, 82, 75, 999, 68, 69], \n",
    "        'SEX': [1, 1, 2, 1, 1, 1, 2, 1, 2, 1], \n",
    "        'EDUC': [12, 12, 16, 8, 8, 8, 20, 99, 14, 16],\n",
    "        'MARISTAT': [1, 1, 2, 1, 1, 5, 5, 9, 2, 1], \n",
    "        'RACE': [1, 1, 1, 2, 2, 2, 5, 99, 1, 1], \n",
    "        'INRELTO': [1, 1, 2, 9, 3, 3, 3, 1, 1, 2],\n",
    "        'CVHATT': [0, 0, 1, 2, 2, 2, 0, 9, 0, 1], \n",
    "        'CBSTROKE': [0, 0, 0, 1, 1, 1, 0, 9, 0, 0], \n",
    "        'DIABETES': [0, 1, 2, 1, 1, 1, 0, 9, 1, 0],\n",
    "        'HYPERTEN': [1, 1, 1, 0, 0, 0, 0, 9, 1, 1], \n",
    "        'HYPERCHO': [1, 1, 0, 2, 2, 9, 0, 0, 1, 0], \n",
    "        'TBI': [0, 0, 0, 0, 0, 0, 0, 9, 1, 0], \n",
    "        'DEP2YRS': [0, 0, 1, 0, 0, 0, 0, 9, 1, 0], \n",
    "        'NACCBMI': [25.1, 25.5, 28.9, 32.0, 32.1, 32.2, 22.4, 888.8, 26.0, 27.0],\n",
    "        'DEMENTED': [0, 0, 1, 1, 1, 1, 0, 1, 0, 1]\n",
    "    })\n",
    "    print(f\"Loaded dummy data for demonstration. Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce266c4",
   "metadata": {},
   "source": [
    "# 2. Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Feature Sets ---\n",
    "# Based on hackathon rules: A1, A2, A5, B1 are allowed.\n",
    "\n",
    "# Numerical features that will be imputed (median) and scaled\n",
    "numeric_features = [\n",
    "    'NACCAGE',  # Subject's age\n",
    "    'EDUC',     # Subject's years of education\n",
    "    'NACCBMI'   # Subject's Body Mass Index\n",
    "]\n",
    "\n",
    "# Categorical features that will be imputed (most frequent) and one-hot encoded\n",
    "categorical_features = [\n",
    "    'SEX',      # Subject's sex\n",
    "    'MARISTAT', # Marital status\n",
    "    'RACE',     # Subject's race\n",
    "    'INRELTO'   # Co-participant's relationship to subject\n",
    "]\n",
    "\n",
    "# Self-reported health history (Form A5)\n",
    "# We will binarize these and engineer a new feature\n",
    "health_history_features = [\n",
    "    [cite_start]'CVHATT',   # Heart attack [cite: 32]\n",
    "    [cite_start]'CBSTROKE', # Stroke [cite: 32]\n",
    "    [cite_start]'DIABETES', # Diabetes [cite: 34]\n",
    "    [cite_start]'HYPERTEN', # Hypertension [cite: 34]\n",
    "    [cite_start]'HYPERCHO', # Hypercholesterolemia [cite: 34]\n",
    "    [cite_start]'TBI',      # Traumatic Brain Injury [cite: 34]\n",
    "    [cite_start]'DEP2YRS'   # Depression in last 2 years [cite: 36]\n",
    "]\n",
    "\n",
    "# --- 2. Clean Missing/Unknown Values ---\n",
    "# We replace all non-standard \"missing\" or \"unknown\" codes with np.nan\n",
    "# This is critical for scikit-learn's imputers to work.\n",
    "missing_values_map = {\n",
    "    [cite_start]'NACCAGE': [999],           # [cite: 420]\n",
    "    [cite_start]'EDUC': [99],               # [cite: 367]\n",
    "    [cite_start]'NACCBMI': [888.8, 888],    # [cite: 1118]\n",
    "    [cite_start]'MARISTAT': [9],            # [cite: 377]\n",
    "    [cite_start]'RACE': [99],               # [cite: 295]\n",
    "    [cite_start]'INRELTO': [9],             # [cite: 588]\n",
    "    # For Form A5, 9 means \"Unknown\"\n",
    "    [cite_start]'CVHATT': [9], 'CBSTROKE': [9], 'DIABETES': [9], 'HYPERTEN': [9], # [cite: 1094, 1098, 1105, 1106]\n",
    "    [cite_start]'HYPERCHO': [9], 'TBI': [9], 'DEP2YRS': [9] # [cite: 1107, 1102, 1115]\n",
    "}\n",
    "\n",
    "for col, missing_vals in missing_values_map.items():\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].replace(missing_vals, np.nan)\n",
    "\n",
    "print(\"Replaced special 'missing' codes with NaN.\")\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "# 3a. Binarize Health History\n",
    "# We map 0=Absent to 0, and 1=Recent/Active or 2=Remote/Inactive to 1 (Present).\n",
    "for col in health_history_features:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].map({0: 0, 1: 1, 2: 1})\n",
    "\n",
    "# 3b. Create 'ComorbidityCount'\n",
    "# This counts how many conditions are present.\n",
    "# We fill NaNs with 0 (assuming 'unknown' means 'absent') before summing.\n",
    "data['ComorbidityCount'] = data[health_history_features].fillna(0).sum(axis=1)\n",
    "\n",
    "# Add our new engineered feature to the numeric list\n",
    "numeric_features.append('ComorbidityCount')\n",
    "print(\"Engineered 'ComorbidityCount' feature.\")\n",
    "\n",
    "# --- 4. Separate Features (X) and Target (y) ---\n",
    "if TARGET_COLUMN not in data.columns:\n",
    "    print(f\"FATAL ERROR: Target column '{TARGET_COLUMN}' not found!\")\n",
    "else:\n",
    "    # X contains the raw features to be processed\n",
    "    X = data[numeric_features + categorical_features]\n",
    "    # y contains the final target\n",
    "    y = data[TARGET_COLUMN]\n",
    "    # groups contains the Patient IDs for the split\n",
    "    groups = data[GROUP_COLUMN]\n",
    "\n",
    "    # --- 5. Define Preprocessing Pipelines ---\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # --- 6. Create the ColumnTransformer ---\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_pipeline, numeric_features),\n",
    "            ('cat', categorical_pipeline, categorical_features)\n",
    "        ],\n",
    "        remainder='drop' # Drop any columns we didn't explicitly select\n",
    "    )\n",
    "\n",
    "    # --- 7. Apply Preprocessing ---\n",
    "    # Fit and transform the entire dataset to prepare it\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Get the new feature names (e.g., after one-hot encoding)\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    \n",
    "    # Convert the processed data back to a DataFrame (useful for SHAP later)\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "    print(\"\\n--- Preprocessing Complete ---\")\n",
    "    print(f\"Shape of processed features (X): {X_processed.shape}\")\n",
    "    print(f\"Shape of target (y): {y.shape}\")\n",
    "    print(f\"Shape of groups (groups): {groups.shape}\")\n",
    "    print(\"\\nProcessed features (first 5 rows):\")\n",
    "    print(X_processed_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
